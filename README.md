# Ollama Web Interface

Interface web moderne et accessible pour interagir avec des modÃ¨les d'IA locaux via Ollama.

## ğŸ“‹ Table des matiÃ¨res

- [AperÃ§u](#aperÃ§u)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture](#architecture)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Bonnes pratiques implÃ©mentÃ©es](#bonnes-pratiques-implÃ©mentÃ©es)
- [Technologies utilisÃ©es](#technologies-utilisÃ©es)
- [DÃ©veloppement](#dÃ©veloppement)
- [Licence](#licence)

## ğŸ¯ AperÃ§u

Ce projet est une interface web complÃ¨te permettant d'interagir avec des modÃ¨les d'intelligence artificielle hÃ©bergÃ©s localement via Ollama. L'application offre une expÃ©rience utilisateur fluide et professionnelle pour discuter avec diffÃ©rents modÃ¨les LLM.

## âœ¨ FonctionnalitÃ©s

- **SÃ©lection de modÃ¨les** : Affichage dynamique des modÃ¨les Ollama installÃ©s
- **Interface de chat intuitive** : Conversation en temps rÃ©el avec l'IA
- **Indicateur de statut** : VÃ©rification de la connexion Ã  Ollama
- **Formatage du code** : DÃ©tection et mise en forme automatique des blocs de code
- **Design responsive** : Interface adaptÃ©e mobile et desktop
- **AccessibilitÃ©** : Respect des normes WCAG (labels ARIA, navigation clavier)
- **Gestion d'erreurs** : Messages clairs en cas de problÃ¨me

## ğŸ—ï¸ Architecture

### Structure du projet

```
ollama-web-interface/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ server.js          # Serveur Express (proxy API)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html         # Structure HTML
â”‚   â”œâ”€â”€ style.css          # Styles CSS
â”‚   â””â”€â”€ script.js          # Logique JavaScript
â”œâ”€â”€ package.json           # DÃ©pendances Node.js
â””â”€â”€ README.md             # Documentation
```

### SÃ©paration des responsabilitÃ©s

**Backend (Node.js + Express)**
- Proxy entre le frontend et l'API Ollama
- Gestion du CORS
- Validation des requÃªtes
- Logging des erreurs

**Frontend (HTML/CSS/JS Vanilla)**
- Interface utilisateur
- Gestion de l'Ã©tat de l'application
- Communication avec le backend via fetch API
- Rendu dynamique des messages

### Flux de donnÃ©es

```
Frontend (navigateur)
    â†“ HTTP Request
Backend (Express :3000)
    â†“ HTTP Request
Ollama API (localhost:11434)
    â†“ Response
Backend
    â†“ Response
Frontend
```

## ğŸ“¦ PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© :

- **Node.js** (version 18 ou supÃ©rieure)
- **Ollama** ([TÃ©lÃ©charger ici](https://ollama.com/download))
- Au moins un modÃ¨le Ollama installÃ©

### Installation d'Ollama et d'un modÃ¨le

```bash
# Sur macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# TÃ©lÃ©charger un modÃ¨le (exemple : qwen2.5-coder)
ollama pull qwen2.5-coder

# Lancer Ollama en arriÃ¨re-plan
ollama serve
```

## ğŸš€ Installation

### 1. Cloner ou tÃ©lÃ©charger le projet

```bash
# Si vous avez Git
git clone [URL_DU_REPO]
cd ollama-web-interface

# Ou dÃ©compresser l'archive tÃ©lÃ©chargÃ©e
```

### 2. Installer les dÃ©pendances

```bash
npm install
```

### 3. VÃ©rifier qu'Ollama est lancÃ©

```bash
# Dans un autre terminal
ollama serve
```

### 4. DÃ©marrer le serveur

```bash
npm start
```

Le serveur dÃ©marre sur **http://localhost:3000**

## ğŸ’» Utilisation

1. **Ouvrir l'application** : AccÃ©dez Ã  http://localhost:3000 dans votre navigateur

2. **VÃ©rifier le statut** : L'indicateur en haut doit afficher "ConnectÃ© Ã  Ollama" en vert

3. **SÃ©lectionner un modÃ¨le** : Choisissez un modÃ¨le dans la liste dÃ©roulante

4. **Commencer Ã  discuter** : Tapez votre message et appuyez sur EntrÃ©e (ou Maj+EntrÃ©e pour une nouvelle ligne)

### Raccourcis clavier

- **EntrÃ©e** : Envoyer le message
- **Maj + EntrÃ©e** : Nouvelle ligne dans le message

## âœ… Bonnes pratiques implÃ©mentÃ©es

### Architecture

- âœ… SÃ©paration claire front/back
- âœ… Architecture modulaire avec fonctions rÃ©utilisables
- âœ… Gestion centralisÃ©e des erreurs
- âœ… Configuration via variables (API_URL)

### SÃ©curitÃ©

- âœ… Validation des entrÃ©es cÃ´tÃ© backend
- âœ… Protection contre les injections XSS (escapeHtml)
- âœ… Gestion sÃ©curisÃ©e du CORS
- âœ… Pas de donnÃ©es sensibles exposÃ©es

### AccessibilitÃ© (WCAG)

- âœ… SÃ©mantique HTML5 (`<main>`, `<section>`, `<header>`)
- âœ… Labels ARIA pour les Ã©lÃ©ments interactifs
- âœ… Attributs `role`, `aria-live`, `aria-label`
- âœ… Classe `.visually-hidden` pour les screen readers
- âœ… Navigation au clavier complÃ¨te
- âœ… Contraste des couleurs respectÃ©
- âœ… Focus visible sur les Ã©lÃ©ments interactifs

### Performances

- âœ… CSS avec variables pour rÃ©utilisation
- âœ… Pas de framework lourd (Vanilla JS)
- âœ… Ã‰vÃ©nements optimisÃ©s (pas de fuites mÃ©moire)
- âœ… RequÃªtes HTTP asynchrones avec async/await
- âœ… Scroll automatique uniquement si nÃ©cessaire

### MaintenabilitÃ©

- âœ… Code commentÃ© et documentÃ©
- âœ… Nommage clair des variables et fonctions
- âœ… Structure de fichiers logique
- âœ… SÃ©paration des prÃ©occupations (HTML/CSS/JS)
- âœ… Constantes en dÃ©but de fichier
- âœ… Fonctions courtes et spÃ©cialisÃ©es

### UX/UI

- âœ… Design moderne et Ã©purÃ©
- âœ… Feedback visuel (loading, status)
- âœ… Messages d'erreur clairs
- âœ… Interface responsive (mobile/desktop)
- âœ… Animation fluide et subtile
- âœ… Ã‰tats dÃ©sactivÃ©s visuellement clairs

## ğŸ› ï¸ Technologies utilisÃ©es

### Backend
- **Node.js** : Environnement d'exÃ©cution JavaScript
- **Express** : Framework web minimaliste
- **CORS** : Middleware pour gÃ©rer les requÃªtes cross-origin

### Frontend
- **HTML5** : Structure sÃ©mantique
- **CSS3** : Styles modernes (Grid, Flexbox, Variables CSS)
- **JavaScript Vanilla** : Logique applicative (ES6+)
- **Fetch API** : Communication HTTP

### Outils de dÃ©veloppement
- **Nodemon** (optionnel) : Rechargement automatique du serveur

## ğŸ‘¨â€ğŸ’» DÃ©veloppement

### Mode dÃ©veloppement avec rechargement automatique

```bash
npm run dev
```

### Structure des requÃªtes API

**GET /api/health**
```json
{
  "status": "ok",
  "ollama": "connected",
  "message": "Ollama est accessible"
}
```

**GET /api/models**
```json
{
  "models": [
    {
      "name": "qwen2.5-coder",
      "size": 4711348672
    }
  ]
}
```

**POST /api/chat**
```json
// Request
{
  "model": "qwen2.5-coder",
  "prompt": "Explique-moi le concept de closure en JavaScript"
}

// Response
{
  "response": "Une closure est...",
  "done": true
}
```

### Personnalisation

#### Changer le port du serveur

Modifier dans `backend/server.js` :
```javascript
const PORT = 3000; // Changer ici
```

#### Modifier l'URL de l'API Ollama

Modifier dans `backend/server.js` :
```javascript
const OLLAMA_API_URL = 'http://localhost:11434'; // Changer ici
```

#### Personnaliser les couleurs

Modifier les variables CSS dans `frontend/style.css` :
```css
:root {
  --primary-color: #2563eb; /* Couleur principale */
  --success-color: #10b981; /* Couleur de succÃ¨s */
  /* ... */
}
```

## ğŸ› RÃ©solution de problÃ¨mes

### Le serveur ne dÃ©marre pas

- VÃ©rifiez que le port 3000 n'est pas dÃ©jÃ  utilisÃ©
- VÃ©rifiez que Node.js est installÃ© : `node --version`
- RÃ©installez les dÃ©pendances : `rm -rf node_modules && npm install`

### "Ollama non disponible"

- VÃ©rifiez qu'Ollama est lancÃ© : `ollama serve`
- VÃ©rifiez qu'Ollama Ã©coute sur le port 11434
- Testez manuellement : `curl http://localhost:11434/api/tags`

### "Aucun modÃ¨le trouvÃ©"

- Installez un modÃ¨le : `ollama pull qwen2.5-coder`
- Listez les modÃ¨les : `ollama list`

### ProblÃ¨mes de CORS

- VÃ©rifiez que le backend est bien lancÃ©
- VÃ©rifiez l'URL de l'API dans `script.js` (ligne 7)

## ğŸ“ Licence

MIT - Projet rÃ©alisÃ© dans le cadre d'un atelier pÃ©dagogique

## ğŸ‘¤ Auteur

**[Vos noms et prÃ©noms]**

Projet dÃ©veloppÃ© avec l'assistance d'IA gÃ©nÃ©rative dans le cadre du cours "Coder avec l'IA gÃ©nÃ©rative".




# CrÃ©er la structure de dossiers
New-Item -Path "ollama-web-interface" -ItemType Directory
cd ollama-web-interface
New-Item -Path "backend" -ItemType Directory
New-Item -Path "frontend" -ItemType Directory